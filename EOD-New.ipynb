{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685021f6-e33e-437e-b201-3a41e92d5afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from eodhd import APIClient\n",
    "import os\n",
    "import urllib, json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6898e28c-878b-4e7d-bc0e-74ccbeb65883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EOD_API():\n",
    "    #EOD API \n",
    "    YOUR_API_KEY=\"64795e4c5f6224.06757647776\"\n",
    "    api = APIClient(YOUR_API_KEY)\n",
    "    \n",
    "    def read_url(url=\" \"):\n",
    "        response = urllib.request.urlopen(url)\n",
    "        data = json.loads(response.read())\n",
    "        return data\n",
    "    \n",
    "    def data_to_df(data=[]):\n",
    "        df=pd.DataFrame.from_dict(data)\n",
    "    \n",
    "    \n",
    "    def ExchangeList():\n",
    "        dfExchange=read_url(url=f\"https://eodhistoricaldata.com/api/exchanges-list/?api_token={YOUR_API_KEY}\")\n",
    "        return dfExhange\n",
    "    \n",
    "    def GlobalTickerList(dfExchange=[]):\n",
    "        exchangeList = dfExchange[\"Code\"].tolist()\n",
    "        dfGlobalTicker=[]\n",
    "        for exchangeCode in exchangeList:\n",
    "            dfTicker = read_url(url=f\"https://eodhistoricaldata.com/api/exchange-symbol-list/{exchangeCode}?api_token={YOUR_API_KEY}&fmt=json\")\n",
    "            dfGlobalTicker.append(dfTicker)\n",
    "        return dfGlobalTicker\n",
    "    \n",
    "    def Tickers_in_exchange(exchangeCode=[]):\n",
    "        \n",
    "        dfTicker = read_url(url=f\"https://eodhistoricaldata.com/api/exchange-symbol-list/{exchangeCode}?api_token={YOUR_API_KEY}&fmt=json\")\n",
    "        return dfTicker\n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "    def companyInfo(exchange=[],tickerList=[]):\n",
    "        #Fundamental Data Keys\n",
    "        allFundamentalDataKeys=['General', 'Highlights', 'Valuation', 'SharesStats', 'Technicals',\n",
    "               'SplitsDividends', 'AnalystRatings', 'Holders', 'InsiderTransactions',\n",
    "               'ESGScores', 'outstandingShares', 'Earnings', 'Financials']\n",
    "    \n",
    "        #Key Sub Classification\n",
    "        #ESGScource - Outdated \n",
    "\n",
    "        companyInfoDataKeys = ['General', 'Highlights', 'Valuation', 'SharesStats', 'Technicals']#, 'SplitsDividends','AnalystRatings']\n",
    "        #managementInfo = it is part of \"General\" - \"Column\"= \"officers\"\n",
    "\n",
    "        shareholdersKey = [\"Holders\"]\n",
    "        iTransactionKey = [\"InsiderTransactions\"]\n",
    "\n",
    "        #3 Parts to Earnings - Historical, Annnual, Trend \n",
    "        # Trend is not useful\n",
    "        earningsKey = [\"Earnings\"]\n",
    "\n",
    "        #Outstanding Shares is multi-period \n",
    "        osSharesKey = ['outstandingShares'] \n",
    "        financialsKey = [\"Financials\"]\n",
    "            \n",
    "        #list of DataFrames for each company details \n",
    "        companyInfo = [] \n",
    "        managementInfo=[]\n",
    "        shareholderInfo = []\n",
    "        insiderTransacInfo = []\n",
    "        earningHist = []\n",
    "        #earningTrend  = []\n",
    "        earningAnnual = [] \n",
    "        annualFS=[]\n",
    "        quarterFS=[]\n",
    "\n",
    "        noData = {}\n",
    "        noDataDf = []\n",
    "        \n",
    "        for ticker in tickerList:\n",
    "            try:\n",
    "                data = read_url(f\"https://eodhistoricaldata.com/api/fundamentals/{ticker}.{exchange}?api_token={YOUR_API_KEY}\")\n",
    "            except:\n",
    "                noDataDf.append(\"No Data\")\n",
    "                noData[ticker] = noDataDf\n",
    "            \n",
    "            try:\n",
    "                #Company Info\n",
    "                ciL = []\n",
    "                for key in companyInfoDataKeys:\n",
    "                    ciL.append(pd.DataFrame.from_dict([data[key]]))\n",
    "\n",
    "            \n",
    "                dfCI=pd.concat(ciL,axis=1)\n",
    "                dfCI.columns = dfCI.columns.map(convert_to_space_upper)\n",
    "\n",
    "                dfOff = pd.DataFrame.from_dict(dfCI.copy()[\"OFFICERS\"][0]).transpose()\n",
    "                dfOff[\"TICKER\"] = ticker\n",
    "                dfCI = dfCI.drop([\"ADDRESS DATA\",\"OFFICERS\",\"LISTINGS\"],axis=1)\n",
    "                dfCI.rename(columns={\"CODE\": \"TICKER\", \"COUNTRY NAME\": \"COUNTRY\"},inplace=True)\n",
    "                companyInfo.append(dfCI)\n",
    "                managementInfo.append(dfOff)\n",
    "            \n",
    "            except:\n",
    "                noDataDf.append(\"No Company Info\")\n",
    "                noData[ticker] = noDataDf\n",
    "                \n",
    "            try:\n",
    "                #Company Financials \n",
    "                dfF= []\n",
    "                statements = ['Balance_Sheet','Cash_Flow','Income_Statement']\n",
    "                periods = [\"quarterly\",\"yearly\"]\n",
    "                for period in periods:\n",
    "                    for statement in statements:\n",
    "                        dfFin=pd.DataFrame.from_dict(data['Financials'][statement][period]).transpose()\n",
    "\n",
    "                        if statement not in [\"Balance_Sheet\"]:\n",
    "                            dfFin = dfFin.drop([\"filing_date\",\"currency_symbol\"],axis=1)\n",
    "\n",
    "                        dfF.append(dfFin)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    dfFS=dfF[0].merge(dfF[1],left_on=\"date\",right_on=\"date\").merge(dfF[2],left_on=\"date\",right_on=\"date\")\n",
    "\n",
    "                    #removing Duplicate Columns\n",
    "                    merged_df = dfFS\n",
    "                    # Step 1: Identify columns to drop\n",
    "                    drop_columns = [col for col in merged_df.columns if col.endswith('_y')]\n",
    "\n",
    "                    # Step 2: Rename columns ending with '_x'\n",
    "                    renamed_columns = {col: col[:-2] for col in merged_df.columns if col.endswith('_x')}\n",
    "                    merged_df = merged_df.rename(columns=renamed_columns)\n",
    "\n",
    "                    # Step 3: Drop identified '_y' columns\n",
    "                    merged_df = merged_df.drop(columns=drop_columns)\n",
    "\n",
    "                    # Step 4: Rename remaining columns by removing '_x' suffix\n",
    "                    merged_df.columns = [col[:-2] if col.endswith('_x') else col for col in merged_df.columns]\n",
    "\n",
    "                    dfFS = merged_df\n",
    "\n",
    "\n",
    "                    dfFS[\"YEAR\"] = dfFS[\"date\"].astype(\"datetime64[ns]\").dt.year \n",
    "                    dfFS[\"Ticker\"]=ticker\n",
    "\n",
    "                    if period==\"yearly\":\n",
    "                        periodSh=\"annual\"\n",
    "                    else:\n",
    "                        periodSh = \"quarterly\"\n",
    "\n",
    "                    dfSh=pd.DataFrame.from_dict(data['outstandingShares'][periodSh]).transpose()\n",
    "\n",
    "\n",
    "                    if period == \"quarterly\":\n",
    "                          dfShs = dfSh.copy()[[\"dateFormatted\",\"shares\"]]\n",
    "                          dfShs.columns = [\"date\",\"shares\"]\n",
    "                          dfFSm=pd.merge(dfFS,dfShs,left_on=\"date\",right_on=\"date\")\n",
    "                          quarterFS.append(dfFSm)\n",
    "                          dfF = []\n",
    "                    else:\n",
    "                        dfSh[\"YEAR\"] = dfSh[\"dateFormatted\"].astype(\"datetime64[ns]\").dt.year              \n",
    "                        dfShs=dfSh.copy()[[\"YEAR\",\"shares\"]]\n",
    "                        dfFSm=pd.merge(dfFS,dfShs,left_on=\"YEAR\",right_on=\"YEAR\")\n",
    "                        annualFS.append(dfFSm)\n",
    "                        dfF = []\n",
    "            except:\n",
    "                noDataDf.append(\"No Company Financials\")\n",
    "                noData[ticker] = noDataDf\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                #Insider Transactions\n",
    "                dfInsider=pd.DataFrame.from_dict(data[\"InsiderTransactions\"]).transpose()\n",
    "                dfInsider[\"TICKER\"] = ticker\n",
    "                insiderTransacInfo.append(dfInsider)\n",
    "            except:\n",
    "                noDataDf.append(\"No Insider Transactions\")\n",
    "                noData[ticker] = noDataDf\n",
    "            \n",
    "            try:\n",
    "                #Earnings - Historical and Annual\n",
    "                dfEarningHistorical=pd.DataFrame.from_dict(data[\"Earnings\"][\"Annual\"]).transpose().reset_index(drop=True)\n",
    "                dfEarningHistorical[\"TICKER\"] = ticker\n",
    "                earningHist.append(dfEarningHistorical)\n",
    "\n",
    "                dfEarningAnnual=pd.DataFrame.from_dict(data[\"Earnings\"][\"Annual\"]).transpose().reset_index(drop=True)\n",
    "                dfEarningAnnual[\"TICKER\"] = ticker\n",
    "                earningAnnual.append(dfEarningAnnual)\n",
    "            except:\n",
    "                noDataDf.append(\"No Earnings Data\")\n",
    "                noData[ticker] = noDataDf\n",
    "            \n",
    "            try:\n",
    "                #Shareholders\n",
    "                dfHolderInsti=pd.DataFrame.from_dict(data[\"Holders\"][\"Institutions\"]).transpose()\n",
    "                dfHolderInsti[\"Holding Type\"]=\"Institutions\"\n",
    "\n",
    "                dfHolderFunds=pd.DataFrame.from_dict(data[\"Holders\"][\"Funds\"]).transpose()\n",
    "                dfHolderFunds[\"Holding Type\"]=\"Funds\"\n",
    "                dfShareholding=pd.concat([dfHolderInsti,dfHolderFunds],axis=0)\n",
    "                dfShareholding[\"TICKER\"]=ticker\n",
    "                shareholderInfo.append(dfShareholding)\n",
    "\n",
    "            except:\n",
    "                noDataDf.append(\"No Shareholder Info\")\n",
    "                noData[ticker] = noDataDf\n",
    "            \n",
    "            noDataDf = []\n",
    "            \n",
    "\n",
    "        return companyInfo,managementInfo,annualFS,quarterFS,insiderTransacInfo,earningHist,earningAnnual,shareholderInfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e457aae4-6916-4b5b-853a-3068d0709ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_space_upper(text):\n",
    "    import re\n",
    "    if re.match(r'^[A-Za-z0-9_]+$', text):\n",
    "        if re.match(r'^[a-z]+(_[a-z]+)*$', text):\n",
    "            return text.replace('_', ' ').upper()\n",
    "        else:\n",
    "            return re.sub(r'([a-z0-9])([A-Z])', r'\\1 \\2', text).upper()\n",
    "    else:\n",
    "        return text.upper()\n",
    "\n",
    "def concatList(clist=[]):\n",
    "    df=pd.concat(clist,ignore_index=True)\n",
    "    df.columns = df.columns.map(convert_to_space_upper)\n",
    "    colList=df.columns.tolist()\n",
    "    colList.remove(\"TICKER\")\n",
    "    colList.insert(0, \"TICKER\")\n",
    "    df = df.reindex(columns=colList)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d67e1a8-9b98-4ce3-ab12-561771fb2d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Ratios(df=[]):\n",
    "    #dfF = annual or quarterly financials \n",
    "    \n",
    "    dfF = df.copy()\n",
    "    \n",
    "    debt=dfF['LONG TERM DEBT'] + dfF['SHORT LONG TERM DEBT']+dfF['CAPITAL LEASE OBLIGATIONS']\n",
    "\n",
    "    #PER SHARE RATIOS \n",
    "    dfF[\"EPS\"] = dfF[\"NET INCOME\"]/dfF[\"SHARES\"]\n",
    "    dfF[\"TOTAL REVENUE PER SHARE\"] = dfF[\"TOTAL REVENUE\"]/dfF[\"SHARES\"]\n",
    "    dfF[\"FREE CASH FLOW PER SHARE\"] = dfF['FREE CASH FLOW']/dfF[\"SHARES\"]\n",
    "    dfF[\"EBITDA PER SHARE\"] = dfF['EBITDA']/dfF[\"SHARES\"]\n",
    "\n",
    "    #VALUATION RELATED \n",
    "    dfF['NON-OPERATIONS VALUE'] = dfF['CASH'] + dfF['SHORT TERM INVESTMENTS'] + dfF['LONG TERM INVESTMENTS'] - (dfF['MINORITY INTEREST']*-1) - debt\n",
    "    dfF['FAIR VALUE (30)'] = ((dfF['FREE CASH FLOW']*30)+(dfF['NON-OPERATIONS VALUE']))/dfF['SHARES']\n",
    "    dfF['FAIR VALUE (15)'] =((dfF['FREE CASH FLOW']*15)+(dfF['NON-OPERATIONS VALUE']))/dfF['SHARES']\n",
    "    dfF['FAIR VALUE (45)'] =((dfF['FREE CASH FLOW']*45)+(dfF['NON-OPERATIONS VALUE']))/dfF['SHARES']\n",
    "    dfF[\"Effective Interest Rate\"] = dfF[\"INTEREST EXPENSE\"]/(debt)\n",
    "    dfF[\"Effective Tax Rate\"] =dfF[\"INCOME TAX EXPENSE\"]/dfF[\"INCOME BEFORE TAX\"] \n",
    "    dfF[\"DEBT %\"] = debt/(debt+dfF[\"TOTAL STOCKHOLDER EQUITY\"])\n",
    "    dfF[\"Equity %\"] = 1 - dfF[\"DEBT %\"]\n",
    "\n",
    "\n",
    "    #PROFIT MARGINS \n",
    "    dfF['Net Profit Margin'] = dfF['NET INCOME']/dfF['TOTAL REVENUE']                     \n",
    "    dfF['Operating Profit Margin'] = dfF['OPERATING INCOME']/dfF['TOTAL REVENUE']\n",
    "    dfF['EBITDA Margin'] = dfF['EBITDA']/dfF['TOTAL REVENUE']\n",
    "    dfF['Gross Profit Margin'] = dfF['GROSS PROFIT']/dfF['TOTAL REVENUE']\n",
    "    dfF['DATE']=pd.to_datetime(dfF['DATE']).dt.date\n",
    "\n",
    "\n",
    "    #ACTIVITY/TURNOVER RATIOS:\n",
    "    dfF[\"INVENTORY TURNOVER\"]=dfF[\"COST OF REVENUE\"]/dfF[\"INVENTORY\"]\n",
    "    dfF[\"Days of inventory on hand (DOH)\"]=365/dfF[\"INVENTORY TURNOVER\"]\n",
    "    dfF[\"RECEIVABLES TURNOVER\"] = dfF[\"TOTAL REVENUE\"]/dfF[\"NET RECEIVABLES\"]\n",
    "    dfF[\"Days of sales outstanding (DSO)\"]=365/dfF[\"RECEIVABLES TURNOVER\"]\n",
    "    dfF[\"PAYABLES TURNOVER\"] = dfF[\"COST OF REVENUE\"]/dfF[\"ACCOUNTS PAYABLE\"]\n",
    "    dfF[\"Number of days of payables\"] = 365/dfF[\"PAYABLES TURNOVER\"]\n",
    "    dfF[\"WORKING CAPITAL TURNOVER\"] = dfF[\"TOTAL REVENUE\"]/dfF[\"NET WORKING CAPITAL\"]\n",
    "    dfF[\"FIXED ASSET TURNOVER\"] = dfF[\"TOTAL REVENUE\"]/dfF[\"NET TANGIBLE ASSETS\"]\n",
    "    dfF[\"TOTAL ASSET TURNOVER\"] = dfF[\"TOTAL REVENUE\"]/dfF[\"TOTAL ASSETS\"]\n",
    "\n",
    "\n",
    "    #LIQUIDITY RATIOS \n",
    "    dfF['CURRENT RATIO'] = dfF['TOTAL CURRENT ASSETS']/dfF['TOTAL CURRENT LIABILITIES']\n",
    "    dfF['QUICK RATIO'] = (dfF['CASH'] + dfF['SHORT TERM INVESTMENTS'] + dfF[\"NET RECEIVABLES\"])/dfF['TOTAL CURRENT LIABILITIES']\n",
    "    dfF[\"CASH RATIO\"]=  (dfF['CASH'] + dfF['SHORT TERM INVESTMENTS'])/dfF['TOTAL CURRENT LIABILITIES']                                                                                              \n",
    "    dfF[\"Cash conversion cycle\"] = dfF[\"Days of inventory on hand (DOH)\"]+dfF[\"Days of sales outstanding (DSO)\"]-dfF[\"Number of days of payables\"]\n",
    "\n",
    "    #SOLVENCY & COVERAGE RATIOS\n",
    "    dfF['Debt to Equity'] = (debt)/dfF['TOTAL STOCKHOLDER EQUITY']\n",
    "    dfF[\"DEBT TO ASSETS\"] = dfF[\"TOTAL ASSETS\"]/debt\n",
    "    dfF[\"FINANCIAL LEVERAGE\"]=dfF[\"TOTAL ASSETS\"]/dfF['TOTAL STOCKHOLDER EQUITY']\n",
    "    \n",
    "    dfF[\"INTEREST COVERAGE\"] = dfF[\"EBIT\"]/dfF[\"INTEREST EXPENSE\"]\n",
    "\n",
    "\n",
    "    #RETURN ON CAPITAL RATIOS \n",
    "    dfF['ROIC'] = dfF['EBIT']/(debt + dfF['TOTAL STOCKHOLDER EQUITY'])\n",
    "    dfF[\"ROA\"] = dfF[\"NET INCOME\"]/dfF[\"TOTAL ASSETS\"]\n",
    "    dfF[\"Operating ROA\"] = dfF[\"EBIT\"]/dfF[\"TOTAL ASSETS\"]\n",
    "    dfF[\"ROE\"] = dfF[\"NET INCOME\"]/dfF['TOTAL STOCKHOLDER EQUITY']\n",
    "\n",
    "\n",
    "    numerics = ['int', 'float']\n",
    "    colnumeric = dfF.select_dtypes(include=numerics).columns\n",
    "    dfF[colnumeric]=dfF[colnumeric].fillna(0)\n",
    "    dfF.replace([np.inf,-np.inf],0,inplace=True)\n",
    "    dfF[colnumeric] = round(dfF[colnumeric],4)\n",
    "    dfF.columns = dfF.columns.str.upper()\n",
    "    dfF.columns = dfF.columns.str.lstrip()\n",
    "    \n",
    "    return dfF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c289375b-cf6a-4c43-9e9b-baa7e5ee2379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#SCREENER DATAFRAME - GROWTH AND RATINGS \n",
    "#dfF = \"Annual Financials\"\n",
    "def multidfC(dfF=[],dfC=[],dfM=[]):\n",
    "    \n",
    "    #GROWTH AND AVERAGE \n",
    "    metdf1=dfF[dfF[\"YEAR\"]==dfF[\"YEAR\"].max()]\n",
    "    metdf2=dfF[dfF[\"YEAR\"]==(dfF[\"YEAR\"].max()-1)]\n",
    "\n",
    "    tickcy=metdf1[\"TICKER\"].unique().tolist()\n",
    "    tickly=metdf2[\"TICKER\"].unique().tolist()\n",
    "\n",
    "    ticknt = []\n",
    "    for tick in tickly:\n",
    "        if tick not in tickcy:\n",
    "            ticknt.append(tick)\n",
    "    metdf2=metdf2[metdf2[\"TICKER\"].isin(ticknt)]\n",
    "    metdf = pd.concat([metdf1,metdf2],axis=0)\n",
    "    diff_cols = metdf.columns.difference(dfC.columns)\n",
    "\n",
    "    #Filter out the columns that are different. You could pass in the df2[diff_cols] directly into the merge as well.\n",
    "    selcols = diff_cols.tolist()+ [\"TICKER\"]\n",
    "    selcolmetdf = metdf[selcols]\n",
    "    metdfC = pd.merge(dfC,selcolmetdf,left_on=\"TICKER\",right_on=\"TICKER\",how=\"left\")   \n",
    "\n",
    "    growth_cols = dfM[dfM['Screener_MultiYear']==\"growth\"][\"Metric\"].unique().tolist() \n",
    "\n",
    "    avg_cols = dfM[dfM['Screener_MultiYear']==\"average\"][\"Metric\"].unique().tolist()  \n",
    "    colListg = growth_cols+[coName,year]\n",
    "    colLista = avg_cols+[coName,year]\n",
    "\n",
    "    year_list=dfF[year].unique().tolist()\n",
    "    year_list.sort(reverse=True)\n",
    "\n",
    "    dfFg = dfF[colListg]\n",
    "    dfF10g=dfFg[dfFg[year].isin(year_list[:11])]\n",
    "    dfF5g=dfFg[dfFg[year].isin(year_list[:6])]\n",
    "    dfF3g=dfFg[dfFg[year].isin(year_list[:4])]\n",
    "    dfF1g=dfFg[dfFg[year].isin(year_list[:2])]\n",
    "    dfF1ge=dfFg[dfFg[year].isin(year_list[1:3])]\n",
    "    grL = [dfF10g,dfF5g,dfF3g,dfF1g,dfF1ge]\n",
    "    dfFa = dfF[colLista]\n",
    "    dfF10a=dfFa[dfFa[year].isin(year_list[:10])]\n",
    "    dfF5a=dfFa[dfFa[year].isin(year_list[:5])]\n",
    "    dfF3a=dfFa[dfFa[year].isin(year_list[:4])]\n",
    "    dfF1a=dfFa[dfFa[year].isin(year_list[:2])]\n",
    "    dfF1ae=dfFa[dfFa[year].isin(year_list[1:3])]\n",
    "    avL = [dfF10a,dfF5a,dfF3a,dfF1a,dfF1ae]\n",
    "\n",
    "\n",
    "    colnameg = [\" 10y-growth\",\" 5y-growth\",\" 3y-growth\",\" 1cy-growth\",\" 1ly-growth\"]\n",
    "    colnameav = [\" 10y-average\",\" 5y-average\",\" 3y-average\",\" 1cy-average\",\" 1ly-average\"]\n",
    "    growthlist = []\n",
    "    count = 0\n",
    "    for yg in grL:\n",
    "        yg=yg.pivot_table(index=coName,columns=year,values=growth_cols).groupby(level=0,axis=1).pct_change(axis=1)\n",
    "        ayg=yg.groupby(level=0,axis=1).mean()\n",
    "        col_list = ayg.columns.tolist()\n",
    "        col_list =  [x + colnameg[count] for x in col_list]\n",
    "        ayg.columns = col_list \n",
    "        growthlist.append(ayg)\n",
    "        count += 1\n",
    "\n",
    "    averagelist = []\n",
    "    countav=0\n",
    "    for ya in avL:\n",
    "        ya=ya.pivot_table(index=coName,columns=year,values=avg_cols)\n",
    "        ayg=ya.groupby(level=0,axis=1).mean()\n",
    "        col_list = ayg.columns.tolist()\n",
    "        col_list =  [x + colnameav[countav] for x in col_list]\n",
    "        ayg.columns = col_list \n",
    "        averagelist.append(ayg)\n",
    "        countav += 1\n",
    "\n",
    "    multilist = growthlist + averagelist\n",
    "    \n",
    "    multiyeardfC = pd.concat(multilist,axis=1,join=\"inner\").reset_index()\n",
    "    multidfC = metdfC.merge(multiyeardfC,left_on=coName,right_on=coName)\n",
    "    \n",
    "    multidfC\n",
    "\n",
    "    colcg=[col for col in multidfC.columns if '1cy-growth' in col]\n",
    "    colclg=[col for col in multidfC.columns if '1ly-growth' in col]\n",
    "    for acol in growth_cols:\n",
    "        try:\n",
    "            multidfC.loc[~multidfC[f'{acol} 1cy-growth'].isin([np.nan,0]),f'{acol} 1y-growth'] = multidfC.loc[~multidfC[f'{acol} 1cy-growth'].isin([np.nan,0]),f'{acol} 1cy-growth']\n",
    "            multidfC.loc[multidfC[f'{acol} 1cy-growth'].isin([np.nan,0]),f'{acol} 1y-growth'] = multidfC.loc[multidfC[f'{acol} 1cy-growth'].isin([np.nan,0]),f'{acol} 1ly-growth']\n",
    "            multidfC.drop([f'{acol} 1cy-growth',f'{acol} 1ly-growth'],axis=1,inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    colca=[col for col in multidfC.columns if '1cy-average' in col]\n",
    "    colcla=[col for col in multidfC.columns if '1ly-average' in col]\n",
    "    for acol in avg_cols:\n",
    "        multidfC.loc[~multidfC[f'{acol} 1cy-average'].isin([np.nan]),f'{acol} 1y-average']=multidfC.loc[~multidfC[f'{acol} 1cy-average'].isin([np.nan]),f'{acol} 1cy-average']\n",
    "        multidfC.loc[multidfC[f'{acol} 1cy-average'].isin([np.nan]),f'{acol} 1y-average']=multidfC.loc[multidfC[f'{acol} 1cy-average'].isin([np.nan]),f'{acol} 1ly-average']\n",
    "        multidfC.drop([f'{acol} 1cy-average',f'{acol} 1ly-average'],axis=1,inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    #RATING \n",
    "    met_list = [rev_type,fcf,roic,nprofit,gm,ebitda_m,npm,d_e,c_r]\n",
    "\n",
    "    colnameg = [\" 10y-growth\",\" 5y-growth\",\" 3y-growth\",\" 1y-growth\"]\n",
    "    colnameav = [\" 10y-average\",\" 5y-average\",\" 3y-average\",\" 1y-average\"]\n",
    "    ratingdF = multidfC.copy()\n",
    "\n",
    "    for metrics in met_list:\n",
    "        for co in colnameg+colnameav:\n",
    "            try:\n",
    "                if metrics in [rev_type,fcf,roic,npm,nprofit]:\n",
    "                        bins = [-100000,-0.1,0,0.07,0.15,0.3,100000]\n",
    "                        label = [-2,-1,1,3,6,9]\n",
    "\n",
    "\n",
    "                elif metrics == c_r:\n",
    "                        bins = [0,0.05,0.2,0.75,1.5,2.5,100000]\n",
    "                        label = [-2,-1,0,1,2,3]\n",
    "\n",
    "\n",
    "                elif metrics ==d_e:\n",
    "                        bins = [0,0.05,1,1.5,3,5,100000]\n",
    "                        label = [-2,-1,0,1,2,3]\n",
    "\n",
    "                else:\n",
    "                    bins = [-100000,-0.1,0,0.1,0.25,0.5,100000]\n",
    "                    label = [-2,-1,0,1,2,3]\n",
    "\n",
    "                ratingdF[f'{metrics}{co}-scale'] = pd.cut(ratingdF[f'{metrics}{co}'],bins=bins,labels=label).astype(\"float\")\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "    yL = [\"10y\",\"5y\",\"3y\",\"1y\"]\n",
    "\n",
    "    for y in yL :\n",
    "        colsa=[col for col in ratingdF.columns if f'{y}-average-scale' in col]\n",
    "        colsg = [col for col in ratingdF.columns if f'{y}-growth-scale' in col]\n",
    "        cols = colsa + colsg\n",
    "        ratingdF[cols]=ratingdF[cols].fillna(0)\n",
    "        ratingdF[f'{y}-Overall Rating']=(ratingdF[cols].sum(axis=1)/57)*10\n",
    "        ratingdF[f'{y}-Avg Rating']=(ratingdF[colsa].sum(axis=1)/30)*10\n",
    "        ratingdF[f'{y}-Growth Rating']=(ratingdF[colsg].sum(axis=1)/27)*10\n",
    "    ratingdF['Fundamental Growth Rating']=round((ratingdF['10y-Growth Rating'] + ratingdF['5y-Growth Rating']*2 + ratingdF['3y-Growth Rating']*3 + ratingdF['1y-Growth Rating']*4)/10,2)\n",
    "    ratingdF['Fundamental Avg Rating']=round((ratingdF['10y-Avg Rating'] + ratingdF['5y-Avg Rating']*2 + ratingdF['3y-Avg Rating']*3 + ratingdF['1y-Avg Rating']*4)/10,2)\n",
    "\n",
    "    ratingdF[\"Revenue Size\"] = pd.cut(ratingdF[rev_type],bins=[-100000000000000,500000000,1000000000,20000000000,100000000000,100000000000000],labels=[0,0.25,0.5,0.75,1]).astype(\"float\")\n",
    "    ratingdF[\"Net Profit Size\"] = pd.cut(ratingdF[rev_type],bins=[-100000000000000,50000000,100000000,2000000000,10000000000,100000000000000],labels=[0,0.25,0.5,0.75,1]).astype(\"float\")\n",
    "    ratingdF[\"EBITDA Size\"] = pd.cut(ratingdF[rev_type],bins=[-100000000000000,200000000,400000000,4000000000,20000000000,100000000000000],labels=[0,0.25,0.5,0.75,1]).astype(\"float\")\n",
    "    ratingdF[\"ASSET Size\"] =  pd.cut(ratingdF[rev_type],bins=[-100000000000000,1500000000,3000000000,60000000000,300000000000,100000000000000],labels=[0,0.25,0.5,0.75,1]).astype(\"float\")\n",
    "    ratingdF[\"Fundamental Size Rating\"] = round(((ratingdF[\"Revenue Size\"]+ratingdF[\"Net Profit Size\"]+ratingdF[\"EBITDA Size\"]+ratingdF[\"ASSET Size\"])/4)*10,2)\n",
    "    ratingdF[\"Fundamental Size Rating\"].fillna(0,inplace=True)\n",
    "    ratingdF['Fundamental Overall Rating']=round((ratingdF['10y-Overall Rating'] + ratingdF['5y-Overall Rating']*2 + ratingdF['3y-Overall Rating']*3 + ratingdF['1y-Overall Rating']*4)/10,2)\n",
    "    ratingdF['Fundamental Overall Rating']=round((9*ratingdF['Fundamental Overall Rating'] + ratingdF[\"Fundamental Size Rating\"])/10,2)\n",
    "\n",
    "    rdf=ratingdF[['TICKER','Fundamental Growth Rating','Fundamental Avg Rating',\"Fundamental Size Rating\",'Fundamental Overall Rating']]\n",
    "    \n",
    "    multidfC=pd.merge(multidfC,rdf,left_on=\"TICKER\",right_on=\"TICKER\",how=\"left\")\n",
    "    multidfC.columns = multidfC.columns.str.upper()\n",
    "    multidfC.columns = multidfC.columns.str.lstrip()\n",
    "\n",
    "\n",
    "\n",
    "    return multidfC \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d2600c-005f-4c9e-9b8a-4cd19fb08390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MongoDB:\n",
    "    from pymongo.mongo_client import MongoClient\n",
    "    from pymongo.server_api import ServerApi\n",
    "    from gridfs import GridFS\n",
    "    uri = \"mongodb+srv://yash:bianca2212@takestock.zhiygnu.mongodb.net/?retryWrites=true&w=majority\"\n",
    "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "    db = client[\"Takestock\"]\n",
    "    \n",
    "    \n",
    "    def insert_df(df=[],collectionName=[]):\n",
    "        fs = GridFS(db, collection=collectionName)\n",
    "        df_bytes = df.to_csv(index=False).encode()\n",
    "        file_id = fs.put(df_bytes, filename=collectionName)\n",
    "    \n",
    "    def bulk_insert_df(dbCollections={}):\n",
    "        for cName,df in dbCollections.items():\n",
    "            MongoDB.insert_df(df,cName)\n",
    "            print(f\"{cName} uploaded to mongodb!\")\n",
    "    \n",
    "    def find_collection(collectionName=[]):\n",
    "        fs = GridFS(db, collection=collectionName)\n",
    "        try:\n",
    "            file = fs.find_one({'filename': collectionName})\n",
    "            df = pd.read_csv(file,index_col=False)\n",
    "        except:\n",
    "            df=[]\n",
    "            print(\"No such collection name!\")\n",
    "        return df\n",
    "    \n",
    "    def bulk_download(dbCollections={}):\n",
    "        dfList=[]\n",
    "        for c in dbCollections:\n",
    "            df=MongoDB.find_collection(c)\n",
    "            dfList.append(df)\n",
    "        return dfList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06741959-7d22-426d-b81c-7b4510da61cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DATA LOAD \n",
    "- Return is a list of dataframe \n",
    "- Use Concat to connect all the dataframes in the list \n",
    "- Convert all the columns to upper \n",
    "- Make \"TICKER\" the first column - as it is the KEY of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a2c2f-77b3-4569-a9c6-9cad35ab1a2a",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ffb7be4-faca-414e-858e-b9811fa04f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "companyInfo,managementInfo,annualFS,quarterFS,insiderTransacInfo,earningHist,earningAnnual,shareholderInfo = EOD_API.companyInfo(exchange=\"US\",tickerList=[\"AAPL\",\"MSFT\",\"TSLA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "368cbf76-74ee-47a0-8ddb-847c36a8d414",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companyInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f8f6126-001b-438e-ab3c-a33c12fee093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dfC\u001b[38;5;241m=\u001b[39m\u001b[43mconcatList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompanyInfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dfOff \u001b[38;5;241m=\u001b[39m concatList(managementInfo)\n\u001b[0;32m      3\u001b[0m dfF\u001b[38;5;241m=\u001b[39mconcatList(annualFS)\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mconcatList\u001b[1;34m(clist)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatList\u001b[39m(clist\u001b[38;5;241m=\u001b[39m[]):\n\u001b[1;32m---> 12\u001b[0m     df\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclist\u001b[49m\u001b[43m,\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mmap(convert_to_space_upper)\n\u001b[0;32m     14\u001b[0m     colList\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[0;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[1;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "\n",
    "dfC=concatList(companyInfo)\n",
    "dfOff = concatList(managementInfo)\n",
    "dfF=concatList(annualFS)\n",
    "dfQ=concatList(quarterFS)\n",
    "dfInsTran = concatList(insiderTransacInfo)\n",
    "dfEHistorical = concatList(earningHist)\n",
    "dfEAnnual = concatList(earningAnnual)\n",
    "#dfSh = []#concatList(shareholderInfo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "80a798a4-e36c-4ee7-ac9d-782dce8f473b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## SPECIFIC DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8dca162a-42ce-48f0-abe1-529dc770ecd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "The resolution lifetime expired after 21.148 seconds: Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLifetimeTimeout\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pymongo\\srv_resolver.py:89\u001b[0m, in \u001b[0;36m_SrvResolver._resolve_uri\u001b[1;34m(self, encapsulate_errors)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__srv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m._tcp.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__fqdn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSRV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__connect_timeout\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pymongo\\srv_resolver.py:43\u001b[0m, in \u001b[0;36m_resolve\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(resolver, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresolve\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# dnspython >= 2\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# dnspython 1.X\u001b[39;00m\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\dns\\resolver.py:1368\u001b[0m, in \u001b[0;36mresolve\u001b[1;34m(qname, rdtype, rdclass, tcp, source, raise_on_no_answer, source_port, lifetime, search)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Query nameservers to find the answer to the question.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m \n\u001b[0;32m   1361\u001b[0m \u001b[38;5;124;03mThis is a convenience function that uses the default resolver\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;124;03mparameters.\u001b[39;00m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_default_resolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrdclass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtcp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraise_on_no_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_port\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\dns\\resolver.py:1204\u001b[0m, in \u001b[0;36mResolver.resolve\u001b[1;34m(self, qname, rdtype, rdclass, tcp, source, raise_on_no_answer, source_port, lifetime, search)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(backoff)\n\u001b[1;32m-> 1204\u001b[0m timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_timeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlifetime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\dns\\resolver.py:988\u001b[0m, in \u001b[0;36mBaseResolver._compute_timeout\u001b[1;34m(self, start, lifetime, errors)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m duration \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m lifetime:\n\u001b[1;32m--> 988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LifetimeTimeout(timeout\u001b[38;5;241m=\u001b[39mduration, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lifetime \u001b[38;5;241m-\u001b[39m duration, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout)\n",
      "\u001b[1;31mLifetimeTimeout\u001b[0m: The resolution lifetime expired after 21.148 seconds: Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[242], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mMongoDB\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfrom\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mpymongo\u001b[39;49;00m\u001b[38;5;21;43;01m.\u001b[39;49;00m\u001b[38;5;21;43;01mmongo_client\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mMongoClient\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfrom\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mpymongo\u001b[39;49;00m\u001b[38;5;21;43;01m.\u001b[39;49;00m\u001b[38;5;21;43;01mserver_api\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mimport\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mServerApi\u001b[49m\n",
      "Cell \u001b[1;32mIn[242], line 6\u001b[0m, in \u001b[0;36mMongoDB\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgridfs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridFS\n\u001b[0;32m      5\u001b[0m uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb+srv://yash:bianca2212@takestock.zhiygnu.mongodb.net/?retryWrites=true&w=majority\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mMongoClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mServerApi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m db \u001b[38;5;241m=\u001b[39m client[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTakestock\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_df\u001b[39m(df\u001b[38;5;241m=\u001b[39m[],collectionName\u001b[38;5;241m=\u001b[39m[]):\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pymongo\\mongo_client.py:736\u001b[0m, in \u001b[0;36mMongoClient.__init__\u001b[1;34m(self, host, port, document_class, tz_aware, connect, type_registry, **kwargs)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    733\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mvalidate_timeout_or_none_or_zero(\n\u001b[0;32m    734\u001b[0m         keyword_opts\u001b[38;5;241m.\u001b[39mcased_key(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnecttimeoutms\u001b[39m\u001b[38;5;124m\"\u001b[39m), timeout\n\u001b[0;32m    735\u001b[0m     )\n\u001b[1;32m--> 736\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43muri_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnect_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrv_service_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrv_service_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrv_max_hosts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrv_max_hosts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    746\u001b[0m seeds\u001b[38;5;241m.\u001b[39mupdate(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodelist\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    747\u001b[0m username \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m username\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pymongo\\uri_parser.py:542\u001b[0m, in \u001b[0;36mparse_uri\u001b[1;34m(uri, default_port, validate, warn, normalize, connect_timeout, srv_service_name, srv_max_hosts)\u001b[0m\n\u001b[0;32m    540\u001b[0m connect_timeout \u001b[38;5;241m=\u001b[39m connect_timeout \u001b[38;5;129;01mor\u001b[39;00m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnectTimeoutMS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    541\u001b[0m dns_resolver \u001b[38;5;241m=\u001b[39m _SrvResolver(fqdn, connect_timeout, srv_service_name, srv_max_hosts)\n\u001b[1;32m--> 542\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[43mdns_resolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hosts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m dns_options \u001b[38;5;241m=\u001b[39m dns_resolver\u001b[38;5;241m.\u001b[39mget_options()\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dns_options:\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pymongo\\srv_resolver.py:121\u001b[0m, in \u001b[0;36m_SrvResolver.get_hosts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_hosts\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 121\u001b[0m     _, nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_srv_response_and_hosts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pymongo\\srv_resolver.py:101\u001b[0m, in \u001b[0;36m_SrvResolver._get_srv_response_and_hosts\u001b[1;34m(self, encapsulate_errors)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_srv_response_and_hosts\u001b[39m(\u001b[38;5;28mself\u001b[39m, encapsulate_errors):\n\u001b[1;32m--> 101\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencapsulate_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Construct address tuples\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    105\u001b[0m         (maybe_decode(res\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mto_text(omit_final_dot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)), res\u001b[38;5;241m.\u001b[39mport) \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    106\u001b[0m     ]\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\pymongo\\srv_resolver.py:97\u001b[0m, in \u001b[0;36m_SrvResolver._resolve_uri\u001b[1;34m(self, encapsulate_errors)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# Else, raise all errors as ConfigurationError.\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigurationError(\u001b[38;5;28mstr\u001b[39m(exc))\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[1;31mConfigurationError\u001b[0m: The resolution lifetime expired after 21.148 seconds: Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out.; Server 192.168.0.1 UDP port 53 answered The DNS operation timed out."
     ]
    }
   ],
   "source": [
    "#Defining DataTypes\n",
    "numericCols=dfF.drop([\"TICKER\",\"DATE\",\"FILING DATE\",\"CURRENCY SYMBOL\"], axis=1).columns\n",
    "dfF[numericCols] = dfF[numericCols].astype(\"float\").replace([\"\",np.nan,None],0.00)\n",
    "dfF[numericCols] =dfF[numericCols].astype(\"float\").replace([\"\",np.nan,None],0.00)\n",
    "\n",
    "\n",
    "#Merging columns from Company Info to other Dataframes - this will help in cross-referencing\n",
    "def merge_dfC(df=[],dfC=[]):\n",
    "    dfCsel=dfC[[\"TICKER\",\"NAME\",\"MARKET CAPITALIZATION\",\"SECTOR\",\"INDUSTRY\",\"COUNTRY\"]]\n",
    "    dfmerged = pd.merge(df,dfCsel,left_on=\"TICKER\",right_on=\"TICKER\")\n",
    "    return dfmerged\n",
    "\n",
    "\n",
    "dfF = merge_dfC(dfF,dfC)\n",
    "dfQ = merge_dfC(dfF,dfC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8af23f0-d740-473c-9741-a79f88844fb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. RATIO CALCUALTIONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3388e-f653-4f92-8848-5a89ff4164dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b5edda95-9386-4baa-8d9e-c6a67ee6fa94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfF = Ratios(dfF)\n",
    "dfQ = Ratios(dfQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ce2d00-859e-471e-92fb-a9f78ef5cf0c",
   "metadata": {},
   "source": [
    "# CREATE A MANUAL METRIC REFERENCE FILE FROM ANNUAL OR QUARTER FINANCIALS \n",
    "\n",
    "- WILL SHOW STATEMENT CATEGORY AND SUB-CATEOGRY, AND MULTI-PERIOD FORMATS ETC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca7c6a67-61d0-4067-aeb6-2c3a5c274a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfM = pd.read_csv(r\"C:\\Users\\yash1\\OneDrive\\Desktop\\MetricReference.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8912ef4-45d8-4aa7-a40e-1d789ac59683",
   "metadata": {
    "tags": []
   },
   "source": [
    "# COMMON VARIABLES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "96934165-2245-40f9-bc5d-1a6072b846a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#VARIABLES\n",
    "\n",
    "sector = 'SECTOR'\n",
    "industry = \"INDUSTRY\"\n",
    "ticker = \"TICKER\"\n",
    "coName = \"NAME\"\n",
    "year = \"YEAR\"\n",
    "marketCap='MARKET CAPITALIZATION'\n",
    "updatedTicker=\"YF TICKER\"\n",
    "country = \"COUNTRY\"\n",
    "Date =\"DATE\"\n",
    "roic = 'ROIC'\n",
    "revenue = 'TOTAL REVENUE'\n",
    "rev_type = \"TOTAL REVENUE\"\n",
    "fcf = 'FREE CASH FLOW'\n",
    "gm = 'GROSS PROFIT MARGIN'\n",
    "ebitda_m = 'EBITDA MARGIN'\n",
    "npm = 'NET PROFIT MARGIN'\n",
    "nprofit = \"NET INCOME\"\n",
    "gp = \"GROSS PROFIT\"\n",
    "ebitda = \"EBITDA\"\n",
    "assets = \"TOTAL ASSETS\"\n",
    "liab=\"TOTAL LIAB\"\n",
    "equity=\"TOTAL STOCKHOLDER EQUITY\"\n",
    "cfo = \"TOTAL CASH FROM OPERATING ACTIVITIES\"\n",
    "cfi = \"TOTAL CASHFLOWS FROM INVESTING ACTIVITIES\"\n",
    "cff = \"TOTAL CASH FROM FINANCING ACTIVITIES\"\n",
    "d_e = 'DEBT TO EQUITY'\n",
    "c_r = 'CURRENT RATIO'\n",
    "pe = 'PRICE TO EARNINGS RATIO (TTM)'\n",
    "pcf = 'PRICE TO FREE CASH FLOW (TTM)'\n",
    "prev = \"PRICE TO REVENUE RATIO (TTM)\"\n",
    "st1 = \"IS\"\n",
    "st2 = \"CF\"\n",
    "st3 = \"Ratio\"\n",
    "st4 = \"Ratio\"\n",
    "IS = \"IS\"\n",
    "BS = \"BS\"\n",
    "CF = \"CF\"\n",
    "OT = \"Ratio \"\n",
    "mScale=\"MARKET CAP SCALE\"\n",
    "indexUS=[\"S&P500\",\"NASDAQ100\",\"DOW30\"]\n",
    "indexIND = [\"SECTORIAL INDEX\",\"MARKET CAP INDEX\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168c41a-b15e-43b4-948b-78f688cd766b",
   "metadata": {},
   "source": [
    "# CREATING A MULTI-PERIOD COMPANY INFO - USED FOR SCREENER TABLES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "fc3d17af-98cb-41f6-ae86-e9ac443ff997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#SCREENER DATAFRAME - GROWTH AND RATINGS \n",
    "#dfF = \"Annual Financials\"\n",
    "def multidfC(dfF=[],dfC=[],dfM=[]):\n",
    "    \n",
    "    #GROWTH AND AVERAGE \n",
    "    metdf1=dfF[dfF[\"YEAR\"]==dfF[\"YEAR\"].max()]\n",
    "    metdf2=dfF[dfF[\"YEAR\"]==(dfF[\"YEAR\"].max()-1)]\n",
    "\n",
    "    tickcy=metdf1[\"TICKER\"].unique().tolist()\n",
    "    tickly=metdf2[\"TICKER\"].unique().tolist()\n",
    "\n",
    "    ticknt = []\n",
    "    for tick in tickly:\n",
    "        if tick not in tickcy:\n",
    "            ticknt.append(tick)\n",
    "    metdf2=metdf2[metdf2[\"TICKER\"].isin(ticknt)]\n",
    "    metdf = pd.concat([metdf1,metdf2],axis=0)\n",
    "    diff_cols = metdf.columns.difference(dfC.columns)\n",
    "\n",
    "    #Filter out the columns that are different. You could pass in the df2[diff_cols] directly into the merge as well.\n",
    "    selcols = diff_cols.tolist()+ [\"TICKER\"]\n",
    "    selcolmetdf = metdf[selcols]\n",
    "    metdfC = pd.merge(dfC,selcolmetdf,left_on=\"TICKER\",right_on=\"TICKER\",how=\"left\")   \n",
    "\n",
    "    growth_cols = dfM[dfM['Screener_MultiYear']==\"growth\"][\"Metric\"].unique().tolist() \n",
    "\n",
    "    avg_cols = dfM[dfM['Screener_MultiYear']==\"average\"][\"Metric\"].unique().tolist()  \n",
    "    colListg = growth_cols+[coName,year]\n",
    "    colLista = avg_cols+[coName,year]\n",
    "\n",
    "    year_list=dfF[year].unique().tolist()\n",
    "    year_list.sort(reverse=True)\n",
    "\n",
    "    dfFg = dfF[colListg]\n",
    "    dfF10g=dfFg[dfFg[year].isin(year_list[:11])]\n",
    "    dfF5g=dfFg[dfFg[year].isin(year_list[:6])]\n",
    "    dfF3g=dfFg[dfFg[year].isin(year_list[:4])]\n",
    "    dfF1g=dfFg[dfFg[year].isin(year_list[:2])]\n",
    "    dfF1ge=dfFg[dfFg[year].isin(year_list[1:3])]\n",
    "    grL = [dfF10g,dfF5g,dfF3g,dfF1g,dfF1ge]\n",
    "    dfFa = dfF[colLista]\n",
    "    dfF10a=dfFa[dfFa[year].isin(year_list[:10])]\n",
    "    dfF5a=dfFa[dfFa[year].isin(year_list[:5])]\n",
    "    dfF3a=dfFa[dfFa[year].isin(year_list[:4])]\n",
    "    dfF1a=dfFa[dfFa[year].isin(year_list[:2])]\n",
    "    dfF1ae=dfFa[dfFa[year].isin(year_list[1:3])]\n",
    "    avL = [dfF10a,dfF5a,dfF3a,dfF1a,dfF1ae]\n",
    "\n",
    "\n",
    "    colnameg = [\" 10y-growth\",\" 5y-growth\",\" 3y-growth\",\" 1cy-growth\",\" 1ly-growth\"]\n",
    "    colnameav = [\" 10y-average\",\" 5y-average\",\" 3y-average\",\" 1cy-average\",\" 1ly-average\"]\n",
    "    growthlist = []\n",
    "    count = 0\n",
    "    for yg in grL:\n",
    "        yg=yg.pivot_table(index=coName,columns=year,values=growth_cols).groupby(level=0,axis=1).pct_change(axis=1)\n",
    "        ayg=yg.groupby(level=0,axis=1).mean()\n",
    "        col_list = ayg.columns.tolist()\n",
    "        col_list =  [x + colnameg[count] for x in col_list]\n",
    "        ayg.columns = col_list \n",
    "        growthlist.append(ayg)\n",
    "        count += 1\n",
    "\n",
    "    averagelist = []\n",
    "    countav=0\n",
    "    for ya in avL:\n",
    "        ya=ya.pivot_table(index=coName,columns=year,values=avg_cols)\n",
    "        ayg=ya.groupby(level=0,axis=1).mean()\n",
    "        col_list = ayg.columns.tolist()\n",
    "        col_list =  [x + colnameav[countav] for x in col_list]\n",
    "        ayg.columns = col_list \n",
    "        averagelist.append(ayg)\n",
    "        countav += 1\n",
    "\n",
    "    multilist = growthlist + averagelist\n",
    "    \n",
    "    multiyeardfC = pd.concat(multilist,axis=1,join=\"inner\").reset_index()\n",
    "    multidfC = metdfC.merge(multiyeardfC,left_on=coName,right_on=coName)\n",
    "    \n",
    "    multidfC\n",
    "\n",
    "    colcg=[col for col in multidfC.columns if '1cy-growth' in col]\n",
    "    colclg=[col for col in multidfC.columns if '1ly-growth' in col]\n",
    "    for acol in growth_cols:\n",
    "        try:\n",
    "            multidfC.loc[~multidfC[f'{acol} 1cy-growth'].isin([np.nan,0]),f'{acol} 1y-growth'] = multidfC.loc[~multidfC[f'{acol} 1cy-growth'].isin([np.nan,0]),f'{acol} 1cy-growth']\n",
    "            multidfC.loc[multidfC[f'{acol} 1cy-growth'].isin([np.nan,0]),f'{acol} 1y-growth'] = multidfC.loc[multidfC[f'{acol} 1cy-growth'].isin([np.nan,0]),f'{acol} 1ly-growth']\n",
    "            multidfC.drop([f'{acol} 1cy-growth',f'{acol} 1ly-growth'],axis=1,inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    colca=[col for col in multidfC.columns if '1cy-average' in col]\n",
    "    colcla=[col for col in multidfC.columns if '1ly-average' in col]\n",
    "    for acol in avg_cols:\n",
    "        multidfC.loc[~multidfC[f'{acol} 1cy-average'].isin([np.nan]),f'{acol} 1y-average']=multidfC.loc[~multidfC[f'{acol} 1cy-average'].isin([np.nan]),f'{acol} 1cy-average']\n",
    "        multidfC.loc[multidfC[f'{acol} 1cy-average'].isin([np.nan]),f'{acol} 1y-average']=multidfC.loc[multidfC[f'{acol} 1cy-average'].isin([np.nan]),f'{acol} 1ly-average']\n",
    "        multidfC.drop([f'{acol} 1cy-average',f'{acol} 1ly-average'],axis=1,inplace=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    #RATING \n",
    "    met_list = [rev_type,fcf,roic,nprofit,gm,ebitda_m,npm,d_e,c_r]\n",
    "\n",
    "    colnameg = [\" 10y-growth\",\" 5y-growth\",\" 3y-growth\",\" 1y-growth\"]\n",
    "    colnameav = [\" 10y-average\",\" 5y-average\",\" 3y-average\",\" 1y-average\"]\n",
    "    ratingdF = multidfC.copy()\n",
    "\n",
    "    for metrics in met_list:\n",
    "        for co in colnameg+colnameav:\n",
    "            try:\n",
    "                if metrics in [rev_type,fcf,roic,npm,nprofit]:\n",
    "                        bins = [-100000,-0.1,0,0.07,0.15,0.3,100000]\n",
    "                        label = [-2,-1,1,3,6,9]\n",
    "\n",
    "\n",
    "                elif metrics == c_r:\n",
    "                        bins = [0,0.05,0.2,0.75,1.5,2.5,100000]\n",
    "                        label = [-2,-1,0,1,2,3]\n",
    "\n",
    "\n",
    "                elif metrics ==d_e:\n",
    "                        bins = [0,0.05,1,1.5,3,5,100000]\n",
    "                        label = [-2,-1,0,1,2,3]\n",
    "\n",
    "                else:\n",
    "                    bins = [-100000,-0.1,0,0.1,0.25,0.5,100000]\n",
    "                    label = [-2,-1,0,1,2,3]\n",
    "\n",
    "                ratingdF[f'{metrics}{co}-scale'] = pd.cut(ratingdF[f'{metrics}{co}'],bins=bins,labels=label).astype(\"float\")\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "    yL = [\"10y\",\"5y\",\"3y\",\"1y\"]\n",
    "\n",
    "    for y in yL :\n",
    "        colsa=[col for col in ratingdF.columns if f'{y}-average-scale' in col]\n",
    "        colsg = [col for col in ratingdF.columns if f'{y}-growth-scale' in col]\n",
    "        cols = colsa + colsg\n",
    "        ratingdF[cols]=ratingdF[cols].fillna(0)\n",
    "        ratingdF[f'{y}-Overall Rating']=(ratingdF[cols].sum(axis=1)/57)*10\n",
    "        ratingdF[f'{y}-Avg Rating']=(ratingdF[colsa].sum(axis=1)/30)*10\n",
    "        ratingdF[f'{y}-Growth Rating']=(ratingdF[colsg].sum(axis=1)/27)*10\n",
    "    ratingdF['Fundamental Growth Rating']=round((ratingdF['10y-Growth Rating'] + ratingdF['5y-Growth Rating']*2 + ratingdF['3y-Growth Rating']*3 + ratingdF['1y-Growth Rating']*4)/10,2)\n",
    "    ratingdF['Fundamental Avg Rating']=round((ratingdF['10y-Avg Rating'] + ratingdF['5y-Avg Rating']*2 + ratingdF['3y-Avg Rating']*3 + ratingdF['1y-Avg Rating']*4)/10,2)\n",
    "\n",
    "    ratingdF[\"Revenue Size\"] = pd.cut(ratingdF[rev_type],bins=[-100000000000000,500000000,1000000000,20000000000,100000000000,100000000000000],labels=[0,0.25,0.5,0.75,1]).astype(\"float\")\n",
    "    ratingdF[\"Net Profit Size\"] = pd.cut(ratingdF[rev_type],bins=[-100000000000000,50000000,100000000,2000000000,10000000000,100000000000000],labels=[0,0.25,0.5,0.75,1]).astype(\"float\")\n",
    "    ratingdF[\"EBITDA Size\"] = pd.cut(ratingdF[rev_type],bins=[-100000000000000,200000000,400000000,4000000000,20000000000,100000000000000],labels=[0,0.25,0.5,0.75,1]).astype(\"float\")\n",
    "    ratingdF[\"ASSET Size\"] =  pd.cut(ratingdF[rev_type],bins=[-100000000000000,1500000000,3000000000,60000000000,300000000000,100000000000000],labels=[0,0.25,0.5,0.75,1]).astype(\"float\")\n",
    "    ratingdF[\"Fundamental Size Rating\"] = round(((ratingdF[\"Revenue Size\"]+ratingdF[\"Net Profit Size\"]+ratingdF[\"EBITDA Size\"]+ratingdF[\"ASSET Size\"])/4)*10,2)\n",
    "    ratingdF[\"Fundamental Size Rating\"].fillna(0,inplace=True)\n",
    "    ratingdF['Fundamental Overall Rating']=round((ratingdF['10y-Overall Rating'] + ratingdF['5y-Overall Rating']*2 + ratingdF['3y-Overall Rating']*3 + ratingdF['1y-Overall Rating']*4)/10,2)\n",
    "    ratingdF['Fundamental Overall Rating']=round((9*ratingdF['Fundamental Overall Rating'] + ratingdF[\"Fundamental Size Rating\"])/10,2)\n",
    "\n",
    "    rdf=ratingdF[['TICKER','Fundamental Growth Rating','Fundamental Avg Rating',\"Fundamental Size Rating\",'Fundamental Overall Rating']]\n",
    "    \n",
    "    multidfC=pd.merge(multidfC,rdf,left_on=\"TICKER\",right_on=\"TICKER\",how=\"left\")\n",
    "    multidfC.columns = multidfC.columns.str.upper()\n",
    "    multidfC.columns = multidfC.columns.str.lstrip()\n",
    "\n",
    "\n",
    "\n",
    "    return multidfC \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c332e4-e03d-4995-8743-9694c1a6a244",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CONVERTING NON USD FINANCIALS TO USD \n",
    "- this will require \n",
    "- dfM file\n",
    "- Historical FOREX Data - currently using yfinance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9761ccf1-6cce-4233-b925-b8740d333662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Changing Quarterly Financials to USD\n",
    "def QFUSD(dFF=[],dfM=[]):\n",
    "    import yfinance as yf\n",
    "    dFF.columns = dFF.columns.str.lstrip()\n",
    "    dFF[\"DATE\"]=pd.to_datetime(dFF[\"DATE\"],errors=\"coerce\")\n",
    "   \n",
    "    dFF[\"YEAR\"]=dFF[\"DATE\"].dt.year\n",
    "    dFF[\"QUARTER\"]= dFF[\"DATE\"].dt.quarter\n",
    "    dFF[\"MONTH\"] = dFF[\"DATE\"].dt.month\n",
    " \n",
    "    sd=\"1980-1-1\"\n",
    "    ed = datetime.date.today()\n",
    "\n",
    "\n",
    "    currencyList=dFF[\"CURRENCY SYMBOL\"].unique().tolist()\n",
    "    currencyList.remove(\"USD\")\n",
    "    #currencyList.remove(np.nan)\n",
    "    \n",
    "    currencies=currencyList\n",
    "    currencyTick = []\n",
    "    for currency in currencies:\n",
    "        currencyTick.append(currency+\"USD=X\")\n",
    "\n",
    "    currencyData = yf.download(currencyTick,start=sd,end=ed)\n",
    "    exRates=currencyData[\"Close\"]\n",
    "    exQ = exRates.resample(\"Q\").mean().reset_index()\n",
    "    exQ[\"YEAR\"] = exQ[\"Date\"].dt.year\n",
    "    exQ[\"QUARTER\"] = exQ[\"Date\"].dt.quarter\n",
    "    exM =  exRates.resample(\"M\").mean().reset_index()\n",
    "    exM[\"YEAR\"] = exM[\"Date\"].dt.year\n",
    "    \n",
    "    exM[\"MONTH\"] = exM[\"Date\"].dt.month\n",
    "\n",
    "    dFFex=dFF[dFF[\"CURRENCY SYMBOL\"].isin(currencies)][[\"TICKER\",\"DATE\",\"YEAR\",\"MONTH\",\"QUARTER\",\"CURRENCY SYMBOL\"]]\n",
    "\n",
    "    zlist = []\n",
    "    for currency in currencies:\n",
    "        cCode=currency+\"USD=X\"\n",
    "\n",
    "        x=dFFex[dFFex[\"CURRENCY SYMBOL\"]==currency]\n",
    "\n",
    "        yFp = exQ[[cCode,\"YEAR\",\"QUARTER\"]]\n",
    "        yFp.columns = [\"ExRate_forperiod\",\"YEARfp\",\"QUARTERfp\"]\n",
    "\n",
    "\n",
    "        yAd = exM[[cCode,\"YEAR\",\"MONTH\"]]\n",
    "        yAd.columns=[\"ExRate_asonDate\",\"YEARas\",\"MONTHas\"]\n",
    "\n",
    "        z=pd.merge(x,yFp,left_on=[\"YEAR\",\"QUARTER\"],right_on=[\"YEARfp\",\"QUARTERfp\"])\n",
    "        zqq=pd.merge(z,yAd,left_on=[\"YEAR\",\"MONTH\"],right_on=[\"YEARas\",\"MONTHas\"])\n",
    "\n",
    "        zlist.append(zqq)\n",
    "\n",
    "    erT=pd.concat(zlist)\n",
    "\n",
    "\n",
    "    erT.drop([\"YEAR\",\"MONTH\",\"QUARTER\",\"CURRENCY SYMBOL\",\"YEARfp\",\"QUARTERfp\",\"YEARas\",\"MONTHas\"],axis=1,inplace=True)\n",
    "\n",
    "    dFFex=pd.merge(dFF,erT,left_on=[\"TICKER\",\"DATE\"],right_on=[\"TICKER\",\"DATE\"])\n",
    "\n",
    "    for met in dfM[dfM[\"Statement\"]==\"BS\"][\"Metric\"]:\n",
    "                dFFex[met]=dFFex[met]*dFFex[\"ExRate_asonDate\"]\n",
    "\n",
    "    for met in dfM[dfM[\"Statement\"].isin([\"CF\",\"IS\"])][\"Metric\"]:\n",
    "                dFFex[met]=dFFex[met]*dFFex[\"ExRate_forperiod\"]\n",
    "\n",
    "    usdDF=dFF[~dFF[\"CURRENCY SYMBOL\"].isin(currencies)]\n",
    "\n",
    "    usdF=pd.concat([dFFex,usdDF])\n",
    "\n",
    "    usdF.loc[usdF[\"CURRENCY SYMBOL\"]==\"USD\",\"ExRate_asonDate\"]=1\n",
    "    usdF.loc[usdF[\"CURRENCY SYMBOL\"]==\"USD\",\"ExRate_forperiod\"]=1\n",
    "    \n",
    "    usdF.loc[usdF[\"CURRENCY SYMBOL\"].isin([np.nan]),\"ExRate_asonDate\"]=1\n",
    "    usdF.loc[usdF[\"CURRENCY SYMBOL\"].isin([np.nan]),\"ExRate_forperiod\"]=1\n",
    "    \n",
    "    usdF.dropna(subset=[\"ExRate_forperiod\"],inplace=True)\n",
    "    \n",
    "    return usdF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
